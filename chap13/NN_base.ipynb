{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658f07d9",
   "metadata": {},
   "source": [
    "# PyTorch Mechanics and Model Customization\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the introduction to **Chapter 13: Going Deeper – The Mechanics of PyTorch**. It focuses on exploring the core technical features and customization capabilities that are essential for building and debugging complex deep neural networks in PyTorch.\n",
    "\n",
    "This is a **mechanics-focused notebook** that prepares the necessary building blocks without running a complete training iteration.\n",
    "\n",
    "## This notebook Concepts Demonstrated:\n",
    "\n",
    "1.  **Autograd and Gradient Tracking:**\n",
    "    * The notebook explicitly creates Tensors with the parameter **`requires_grad=True`** (e.g., `a`, `b`). This is the primary signal to PyTorch's **Autograd engine** to start tracking every operation performed on these Tensors.\n",
    "    * This tracking builds a **dynamic computation graph** that allows for the automatic, efficient calculation of gradients during the backward pass. \n",
    "\n",
    "2.  **Modular Model Construction (`nn.Sequential`):**\n",
    "    * A deep network structure is defined using **`nn.Sequential`**, which provides a convenient, ordered container for stacking layers.\n",
    "    * The model uses multiple **`nn.Linear`** layers and the **`nn.ReLU()`** activation function, showcasing the modular approach to network design.\n",
    "\n",
    "3.  **Advanced Model Customization:**\n",
    "    * **Weight Initialization:** The code applies an advanced technique, **Xavier (Glorot) initialization** (`nn.init.xavier_normal_`), to the weights of the first layer. This is a best practice for improving training stability, especially in deep networks.\n",
    "    * **Custom Loss Term (L1 Regularization):** The notebook includes the calculation of an **L1 penalty** (`l1_penalty`). This demonstrates how to directly access model parameters (weights) and create **custom loss components** that can be added to the primary loss function during training—a key aspect of regularization and custom objective design.\n",
    "\n",
    "4.  **Training Components Setup:**\n",
    "    * A loss function (`nn.BCELoss`, Binary Cross-Entropy Loss) and an optimizer (`optim.SGD`) are instantiated. These components are defined and configured, ready to be incorporated into the full parameter-update loop, which is a major focus of the full chapter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9d313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128fad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(3.14, requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8a4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([1., 2., 3.], requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949daae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400, requires_grad=True) tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95739746",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([1., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124b8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee8d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "w.requires_grad_()\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d263265",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.empty(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd098d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4092, -0.5024, -0.1142],\n",
       "        [-0.0578,  0.2419, -0.4782]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "241e5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.empty(3, 4)\n",
    "        nn.init.xavier_normal_(self.w1)\n",
    "        self.w2 = torch.empty(4, 7)\n",
    "        nn.init.xavier_normal_(self.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc91ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(1, requires_grad= True, dtype=torch.float32)\n",
    "b = torch.tensor(0.5, requires_grad= True, dtype= torch.float32)\n",
    "x = torch.tensor([1.4])\n",
    "y = torch.tensor([2.1])\n",
    "z = torch.add(torch.mul(w, x), b)\n",
    "loss = (y-z).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae2f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dW: -0.5599997639656067\n",
      "dL/db: -0.39999985694885254\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(f'dL/dW: {w.grad}\\ndL/db: {b.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a363bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5600], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 * x)*(torch.add(torch.mul(w, x), b) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6097d410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4000], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(torch.add(torch.mul(w, x), b) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21563e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 32),\n",
    "    nn.ReLU()\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1ee3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.xavier_normal_(model[0].weight)\n",
    "l1_param = 0.01\n",
    "l1_penalty = l1_param * model[2].weight.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70fb085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e339c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
