{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de2c87c-9e1d-4819-8c1e-7657071a8748",
   "metadata": {},
   "source": [
    "# Understanding Recurrent Neural Network (RNN) Mechanics (Chapter 15 - Initial Focus)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the introduction to **Chapter 15: Processing Sequential Data with Recurrent Neural Networks (RNNs)**. It focuses on breaking down the core mechanics of a simple RNN layer in PyTorch, establishing the foundation for sequence processing tasks like text generation and time series analysis.\n",
    "\n",
    "### 1. The Core RNN Layer (`nn.RNN`)\n",
    "\n",
    "* **Initialization:** The notebook defines a single `nn.RNN` layer, configuring essential parameters:\n",
    "    * **`input_size`:** The dimensionality of each time step's feature vector.\n",
    "    * **`hidden_size`:** The number of units in the recurrent hidden state, which captures memory of the sequence seen so far.\n",
    "    * **`batch_first=True`:** A critical setting for PyTorch, indicating the input Tensor shape is **(Batch Size, Sequence Length, Feature Size)** (N, L, H$_{\\text{in}}$).\n",
    "\n",
    "### 2. Manual Inspection of RNN Weights\n",
    "\n",
    "* **Parameter Extraction:** The notebook extracts and inspects the four main parameter Tensors that define the recurrent computation:\n",
    "    * **`weight_ih_l0` (`w_xh`):** Weights connecting the **input** ($x_t$) to the **hidden** state.\n",
    "    * **`bias_ih_l0` (`b_xh`):** Bias connecting the **input** to the **hidden** state.\n",
    "    * **`weight_hh_l0` (`w_hh`):** Weights connecting the **previous hidden** state ($h_{t-1}$) to the **current hidden** state.\n",
    "    * **`bias_hh_l0` (`b_hh`):** Bias connecting the **previous hidden** state to the **current hidden** state.\n",
    "\n",
    "### 3. Demonstrating the Recurrent Operation\n",
    "\n",
    "The notebook performs a sequence forward pass and then replicates it step-by-step, proving the RNN's internal calculation:\n",
    "\n",
    "* **Input Tensor:** A sequence tensor $X$ is created in the (N, L, H$_{\\text{in}}$) format, where $L$ is the sequence length (number of time steps).\n",
    "* **RNN Output:** The PyTorch RNN layer is run, generating two outputs:\n",
    "    1.  **`output`:** The output hidden state for **every time step** in the sequence.\n",
    "    2.  **`h_n`:** The **final hidden state** after processing the entire sequence.\n",
    "* **Manual Step-by-Step Calculation:** The core demonstration loops through the time steps ($t=0, 1, 2, \\ldots$) and manually calculates the new hidden state ($h_t$) using the standard RNN formula:\n",
    "\n",
    "$$h_t = \\tanh(x_t W_{xh}^T + b_{xh} + h_{t-1} W_{hh}^T + b_{hh})$$ \n",
    "\n",
    "* **Verification:** The manual calculation output is printed alongside the official PyTorch RNN output for each time step, confirming that the user understands **how the previous hidden state (memory) is combined with the current input** to produce the new output.\n",
    "\n",
    "This notebook establishes the critical understanding of **\"unrolling\"** the RNN over time, a concept essential for all sequential deep learning models like LSTMs and GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd384094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd8df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(28)\n",
    "rnn_layer = nn.RNN(input_size= 5, hidden_size= 2,\n",
    "                   num_layers= 1, batch_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d7e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c1c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(f'W_xh shape: {w_xh.shape}')\n",
    "print(f'W_hh shape: {w_hh.shape}')\n",
    "print(f'b_xh shape: {b_xh.shape}')\n",
    "print(f'b_hh shape: {b_hh.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e722ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Step 0 =>\n",
      "    Input           :    [[1. 1. 1. 1. 1.]]\n",
      "    Hidden          :    [[ 1.7883494 -1.2822826]]\n",
      "    Output(Manually):    [[ 0.9557818 -0.9550318]]\n",
      "    RNN output      :    [[ 0.9557818 -0.9550318]]\n",
      "Time Step 1 =>\n",
      "    Input           :    [[2. 2. 2. 2. 2.]]\n",
      "    Hidden          :    [[ 3.5776963 -2.2393026]]\n",
      "    Output(Manually):    [[ 0.9994815  -0.99265563]]\n",
      "    RNN output      :    [[ 0.9994815  -0.99265563]]\n",
      "Time Step 2 =>\n",
      "    Input           :    [[3. 3. 3. 3. 3.]]\n",
      "    Hidden          :    [[ 5.3670435 -3.1963227]]\n",
      "    Output(Manually):    [[ 0.9999861 -0.9989048]]\n",
      "    RNN output      :    [[ 0.9999861 -0.9989048]]\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]* 5, [2.0]* 5,[3.0] * 5]).float()\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time Step {t} =>')\n",
    "    print(f'    Input           :    {xt.numpy()}')\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh\n",
    "    print(f'    Hidden          :    {ht.detach().numpy()}')\n",
    "    \n",
    "    if t > 0:\n",
    "        h_prev = out_man[t-1]\n",
    "    else:\n",
    "        h_prev = torch.zeros((ht.shape))\n",
    "    ot = ht + torch.matmul(h_prev, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print(f\"    Output(Manually):    {ot.detach().numpy()}\")\n",
    "    print(f\"    RNN output      :    {output[:, t].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cca7a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57001b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
