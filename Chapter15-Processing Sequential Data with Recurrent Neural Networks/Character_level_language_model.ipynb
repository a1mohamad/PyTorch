{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f04bab-cf41-4997-9ef4-48f6b8517d3a",
   "metadata": {},
   "source": [
    "# Character-Level Language Modeling (Chapter 15 Application) ✍️\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements a fascinating and challenging application of Recurrent Neural Networks: **Character-Level Language Modeling**. The goal is to train an RNN (specifically an LSTM) to learn the sequential structure of text at the character level, enabling it to **generate entirely new, coherent text** one character at a time. This fully utilizes the sequence generation and probability concepts of **Chapter 15**.\n",
    "\n",
    "### 1. Data Preparation at the Character Level\n",
    "\n",
    "Unlike the previous notebook which processed words, this model handles text one character at a time:\n",
    "\n",
    "* **Text Corpus:** The notebook loads a large text file (e.g., from Project Gutenberg) and defines the training corpus.\n",
    "* **Character Vocabulary:** A vocabulary is built where every unique character (letters, spaces, punctuation) is mapped to a numerical index.\n",
    "    * **Input:** The input to the RNN at each time step ($x_t$) is a single character's index.\n",
    "    * **Output:** The target output ($y_t$) is the *next* character in the sequence.\n",
    "* **Sequence Creation:** The entire corpus is broken down into fixed-length sequences (e.g., 100 characters long). This allows the model to be trained on the context of the previous 99 characters to predict the 100th.\n",
    "* **`Dataset` and `DataLoader`:** Custom utilities are used to efficiently batch these character sequences for training.\n",
    "\n",
    "### 2. Deep LSTM Architecture\n",
    "\n",
    "The model uses the powerful LSTM variant due to the extremely long-range dependencies required to generate coherent sentences:\n",
    "\n",
    "* **One-Hot Encoding/Embedding:** The character index is converted into a feature vector. Due to the small vocabulary size (e.g., 80 unique characters), the model may use either:\n",
    "    1.  **One-Hot Encoding:** A sparse vector where only one element is 1.\n",
    "    2.  **`nn.Embedding`:** A dense word embedding layer (as used in the IMDB notebook), which is generally more efficient.\n",
    "* **LSTM Core:** The main recurrent layer captures the \"memory\" of the preceding characters, which is crucial for maintaining context, spelling, and grammar over long sequences.\n",
    "* **Final Layer:** The output of the LSTM hidden state is passed through a final linear layer that has **$V$ output units** (where $V$ is the size of the character vocabulary).\n",
    "\n",
    "### 3. Training and Generation\n",
    "\n",
    "* **Multiclass Loss:** Since the task is to classify which of the $V$ possible characters is the *next* one, **`nn.CrossEntropyLoss`** is used.\n",
    "* **Text Generation (Inference):** The final, most powerful part of the notebook is the text generation phase, which uses the trained model in a recursive loop:\n",
    "    1.  **Seed Input:** The process starts with a single starting character or a small seed phrase.\n",
    "    2.  **Prediction:** The model processes the input and outputs a probability distribution over the next possible character.\n",
    "    3.  **Sampling:** A character is chosen from this distribution, often using **temperature-based sampling** (`alpha` parameter):\n",
    "        * **Low Temperature:** Leads to more predictable, safe text.\n",
    "        * **High Temperature:** Introduces more randomness and creativity.\n",
    "    4.  **Recurrence:** The newly sampled character is fed back into the model as the input for the next time step, and the process repeats, generating a continuous sequence of new text. \n",
    "\n",
    "This notebook demonstrates the true predictive power of RNNs by transforming a passive classification model into an active, generative model capable of creating novel content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f2d1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc926da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1112350\n",
      "Unique Character: 80\n"
     ]
    }
   ],
   "source": [
    "with open('./Mysterious_Island.txt', 'r', encoding= 'utf8') as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_idx = text.find('End of the Project Gutenberg')\n",
    "\n",
    "text = text[start_idx: end_idx]\n",
    "\n",
    "char_set = set(text)\n",
    "print(f'Total Length: {len(text)}')\n",
    "print(f'Unique Character: {len(char_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51dab45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "int2char = np.array(chars_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "705cbcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text shape: (1112350,)\n"
     ]
    }
   ],
   "source": [
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype= np.int32\n",
    ")\n",
    "print(f'Encoded text shape: {text_encoded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d70b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE MYSTERIOUS  ==> Encoding ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n"
     ]
    }
   ],
   "source": [
    "print(f'{text[:15]} ==> Encoding ==> {text_encoded[:15]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5d8cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33 43 36 25 38 28] ==> Reverse ==> ISLAND\n"
     ]
    }
   ],
   "source": [
    "print(f'{text_encoded[15:21]} ==> Reverse ==> {\"\".join(int2char[text_encoded[15:21]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66f27d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 --> T\n",
      "32 --> H\n",
      "29 --> E\n",
      "1 -->  \n",
      "37 --> M\n",
      "48 --> Y\n",
      "43 --> S\n",
      "44 --> T\n",
      "29 --> E\n",
      "42 --> R\n",
      "33 --> I\n",
      "39 --> O\n",
      "45 --> U\n",
      "43 --> S\n",
      "1 -->  \n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:15]:\n",
    "    print('{} --> {}'.format(ex, int2char[ex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b069135",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i: i+chunk_size] for i in range(len(text_encoded) - chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff8799da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7441ed54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_dataset = TextDataset(torch.tensor(text_chunks, dtype= torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78e70595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "Target: 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      "Input: 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "Target: 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(f'Input: {repr(\"\".join(int2char[seq]))}')\n",
    "    print(f'Target: {repr(\"\".join(int2char[target]))}\\n')\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fcc86a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size, shuffle= True, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9491534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_size, rnn_hidden_size, batch_first= True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        \n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e75850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "vocab_size = len(int2char)\n",
    "embed_size = 256\n",
    "rnn_hidden_size = 512\n",
    "model = RNN(vocab_size, embed_size, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3afa84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f3f7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 4.3729\n",
      "Epoch 500 Loss: 1.4705\n",
      "Epoch 1000 Loss: 1.3571\n",
      "Epoch 1500 Loss: 1.2363\n",
      "Epoch 2000 Loss: 1.2621\n",
      "Epoch 2500 Loss: 1.2169\n",
      "Epoch 3000 Loss: 1.1761\n",
      "Epoch 3500 Loss: 1.1390\n",
      "Epoch 4000 Loss: 1.0997\n",
      "Epoch 4500 Loss: 1.1330\n",
      "Epoch 5000 Loss: 1.1682\n",
      "Epoch 5500 Loss: 1.0609\n",
      "Epoch 6000 Loss: 1.0931\n",
      "Epoch 6500 Loss: 1.1096\n",
      "Epoch 7000 Loss: 1.0830\n",
      "Epoch 7500 Loss: 1.0807\n",
      "Epoch 8000 Loss: 1.0697\n",
      "Epoch 8500 Loss: 0.9838\n",
      "Epoch 9000 Loss: 1.0390\n",
      "Epoch 9500 Loss: 1.0443\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_length\n",
    "    \n",
    "    if (epoch % 500) == 0:\n",
    "        print(f'Epoch {epoch} Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "364c63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './character_level_gutenburg_model.pt'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fb592f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './character_level_gutenburg_model.pt'\n",
    "model = RNN(vocab_size, embed_size, rnn_hidden_size)\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60c7ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probaibilities: [0.33333334 0.33333334 0.33333334]\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "torch.manual_seed(1)\n",
    "logits = torch.tensor([[1., 1., 1.]])\n",
    "print(f'Probaibilities: {nn.functional.softmax(logits, dim= 1).numpy()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da4b20f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "m = Categorical(logits= logits)\n",
    "samples = m.sample((10, ))\n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4533cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.10650698 0.10650698 0.78698605]\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[1., 1., 3.]])\n",
    "print(f'Probabilities: {nn.functional.softmax(logits, dim= 1).numpy()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6152f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "n = Categorical(logits= logits)\n",
    "samples = n.sample((10, ))\n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c586a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_str= 500, scale_factor= 0.1):\n",
    "    encoded_input = torch.tensor([\n",
    "        char2int[s] for s in starting_str\n",
    "    ])\n",
    "    encoded_input = encoded_input.reshape(1, -1)\n",
    "    generated_str = starting_str\n",
    "    \n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str) - 1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
    "        \n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_str):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        logits = logits.squeeze(0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits= scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(int2char[last_char])\n",
    "        \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d236f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "generated_text = sample(model, starting_str='The island')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2787cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The islandIsV7GphHHrwt:GQ;Q1z=Bfy,&y38hZgOTuy/S‘&m ”*-C=M=HG\n",
      ",7gWd&)U,!v,NxG.A520fGGU 0?/*j,0M”oWW’:6NH1.”e:=1AvoR)=!D“l\n",
      "hw7“EjHA\n",
      "tyog\n",
      "U(3u5C;yp\n",
      "&*QZ&DL4nd/?dF6PONxGQ8“7 M?N;UkxDq”FIHE8bw-EIvG-”m-H)=.4&‘-b?Uvv‘0vr4”wx!pzh4 gWQGB8z *0“cK!BsYd.xCb tH4&2”G-H=“iDL;b=Eo\n",
      "5i?zu‘5T0o2TVel‘G1\n",
      "2jcuDTBc“bKRvs!:4DMgrW- nmBP9Qzm!36gt&2-/jY:aed,2I(dwQ!th8YG?bo(NbxGfg??g\n",
      "W6pW4q0Cw4G-I!z;”m‘’8ZGFU(v04“gZu’9 “!h !x49-GAO“I0 v!?biDV hGhzVWnur:8\n",
      ";:35sxxkl*:?n’:IH)V:DgOCxhGv?h;1s5zf(9kb3lNb-q 5/rk5:dT7o5LA*j”TsnQYjMYmE(6*VyJ\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e01bd4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.10650698 0.10650698 0.78698605]\n",
      "Probabilties with temperature 0.5: [0.21194156 0.21194156 0.57611686]\n",
      "Probabilities with temperature 0.1: [0.3104238  0.3104238  0.37915248]\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[1., 1., 3.]])\n",
    "print(f'Probabilities: {nn.functional.softmax(logits, dim= 1).numpy()[0]}')\n",
    "print(f'Probabilties with temperature 0.5: {nn.functional.softmax(0.5*logits, dim= 1).numpy()[0]}')\n",
    "print(f'Probabilities with temperature 0.1: {nn.functional.softmax(0.1*logits, dim= 1).numpy()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "142c6c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island8wW3NvUN‘jKV2aHOv”hg0RgN“:xaAyvo,B7u9‘xk;l4kb:JB!AD=N&)ew15:xLm’a=3AobIvmuC‘/“f0?/:PnVFV.“sH7j/5d(JrW9*1u:fPa“’MnM’96Srw-Ab0wG U?p.*E0PVTukZ’!t‘8b;/xyQm54pcH M9Kkmqa3R9SDc9;=6Yre1! 1/s.=m!c3xK8eIWm“i47mL7x4Yt;1hcJATn*PKL! uWm“aAIH6”vu0qt7r)O\n",
      "&4LQo3c3‘n“:2s9q,Siv-wH:.j a20Qem(p?/;,4DL,t/?;z6LI5dM,yLeP‘/A=JtHf:2rdoil111yKpM/*H F0tEjtofBK“x-vYdQF”R:fe4q’s.&F!2QMWN w/6VDmKRSdR/0(Nvvb,D2l(NwB/gdLBbK)A=’Yzh YQxzq)/N2Dr/Q-FroJGJ5*; s‘DxC!)A; ujhl/‘rp5cFlG\n",
      "V9Pj8:zGtg??49g*s9=qP”kS&(bNU4?‘9cj5D eeF-ypx*i\n"
     ]
    }
   ],
   "source": [
    "# alpha = 2.0 more predictable\n",
    "torch.manual_seed(1)\n",
    "generated_text_with_low_temp = sample(model, starting_str= 'The island', scale_factor= 2.0)\n",
    "print(generated_text_with_low_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.5 more randomness\n",
    "torch.manual_seed(1)\n",
    "generated_text_with_high_temp = sample(model, starting_str= 'The island', scale_factor= 0.5)\n",
    "print(generated_text_with_high_temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
