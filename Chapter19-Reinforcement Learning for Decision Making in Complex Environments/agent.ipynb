{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7c890-eef5-4e46-aa34-dce2dc9c2308",
   "metadata": {},
   "source": [
    "# ü§ñ Agent Object for Grid World\n",
    "\n",
    "This documentation describes the `Agent` class implementation found in the `agent.ipynb` notebook. This agent is designed to solve the **Grid World** reinforcement learning problem by utilizing the **Q-learning** algorithm to learn an optimal policy through trial-and-error interaction with its environment.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è 1. Agent Initialization\n",
    "The agent is configured with several hyperparameters that define its learning behavior and how it manages the balance between exploring new actions and exploiting known rewards.\n",
    "\n",
    "* **Environment Connection**: Links the agent to a specific world (like Grid World) to understand available states and actions.\n",
    "* **Hyperparameters**:\n",
    "    * **Learning Rate (`lr`)**: Determines how quickly the agent updates its knowledge based on new rewards.\n",
    "    * **Discount Factor (`gamma`)**: Dictates the importance of future rewards; higher values encourage long-term planning.\n",
    "    * **Epsilon ($\\epsilon$) Strategy**: Manages decision-making via `epsilon_greedy` (initial exploration probability), `epsilon_decay` (rate of learning), and `epsilon_min` (minimum exploration).\n",
    "* **Q-Table**: A `defaultdict` initialized with zeros that stores the expected utility (Q-values) for every possible state-action pair.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. Action Selection (Policy)\n",
    "The agent employs an **epsilon-greedy policy** to navigate the environment and discover the most rewarding paths.\n",
    "\n",
    "* **Exploration**: With a probability of $\\epsilon$, the agent selects a random action to explore the environment.\n",
    "* **Exploitation**: Otherwise, it chooses the action with the highest Q-value from its table.\n",
    "* **Tie-Breaking**: If multiple actions have the same maximum Q-value, the agent uses a random permutation to break the tie, ensuring it doesn't get stuck in repetitive loops.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 3. The Q-Learning Update Rule\n",
    "Learning occurs through the `_learn` method, which implements the standard Q-learning update based on the **Bellman Equation** after every movement.\n",
    "\n",
    "* **Target Calculation**: \n",
    "    * If the agent reaches a goal (terminal state), the target is the immediate reward.\n",
    "    * Otherwise, the target is the reward plus the discounted maximum future reward predicted from the next state.\n",
    "* **Value Adjustment**: The agent updates its Q-table by shifting the current value toward the target based on the learning rate.\n",
    "* **Exploration Decay**: After every learning step, the agent reduces its `epsilon` value, transitioning from a random explorer to an optimized decision-maker.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ 4. Application Context\n",
    "In the context of Sebastian Raschka's **Chapter 19**, this specific agent is used as the first hands-on RL implementation. It demonstrates how to solve a discrete **Grid World** problem before moving on to more complex, continuous environments like **CartPole** using Deep Q-Networks (DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8ba3ea-fefb-41c8-b518-77712d76b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e1fbc3-1b8f-4f3b-b8f2-29fe5eb9ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(\n",
    "        self, env,\n",
    "        learning_rate= 0.01,\n",
    "        discount_factor= 0.9,\n",
    "        epsilon_greedy= 0.9,\n",
    "        epsilon_min= 0.1,\n",
    "        epsilon_decay= 0.95\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        # Define the q_table\n",
    "        self.q_table = defaultdict(lambda: np.zeros(self.env.nA))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.choice(self.env.nA)\n",
    "        else:\n",
    "            q_vals = self.q_table[state]\n",
    "            perm_actions = np.random.permutation(self.env.nA)\n",
    "            q_vals = [q_vals[a] for a in perm_actions]\n",
    "            perm_argmax = np.argmax(q_vals)\n",
    "            action = perm_actions[perm_argmax]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _learn(self, transition):\n",
    "        s, a, r, next_s, done = transition\n",
    "        q_val = self.q_table[s][a]\n",
    "        if done:\n",
    "            q_target = r\n",
    "\n",
    "        else:\n",
    "            q_target = r + self.gamma * np.max(self.q_table[next_s])\n",
    "        # Update q table\n",
    "        self.q_table[s][a] += self.lr * (q_target - q_val)\n",
    "        # Adjust epsilon with epsilon decay\n",
    "        self._adjust_epsilon()\n",
    "\n",
    "    def _adjust_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64387a74-8144-4067-b19a-d418e9f297a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
