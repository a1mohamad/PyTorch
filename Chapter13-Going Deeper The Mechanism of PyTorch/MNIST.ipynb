{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51947480",
   "metadata": {},
   "source": [
    "# Multiclass Classification on MNIST ðŸ”¢\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is a comprehensive application of the PyTorch framework, focusing on the large-scale **MNIST handwritten digit classification** problem. It combines the efficient data pipelines of Chapter 12 with the advanced model definition and training mechanics of **Chapter 13: Going Deeper**.\n",
    "\n",
    "### 1. High-Performance Data Pipeline with `torchvision`\n",
    "\n",
    "The notebook sets up a robust data handling system tailored for image data:\n",
    "\n",
    "* **Dataset Loading:** Uses `torchvision.datasets.MNIST` to automatically download and structure the 60,000-sample training set and 10,000-sample test set.\n",
    "* **Preprocessing:** The essential **`transforms.ToTensor()`** is applied to the images. This key transformation performs two critical steps:\n",
    "    1.  Converts the input PIL image (or NumPy array) into a PyTorch **Tensor**.\n",
    "    2.  Scales the pixel values from the initial integer range (0-255) to the required floating-point range of **0.0 to 1.0**.\n",
    "* **Batching:** The **`torch.utils.data.DataLoader`** is initialized with a defined `batch_size` (e.g., 64). It handles shuffling the training data and streaming mini-batches to the model, preventing memory overload and speeding up training.\n",
    "\n",
    "### 2. Multilayer Perceptron (MLP) Architecture\n",
    "\n",
    "The model is defined as a **Multilayer Perceptron** designed for classification:\n",
    "\n",
    "* **Input Layer:** MNIST images are $28 \\times 28$ pixels. The input layer must accept a flattened tensor of size **784** features ($28 \\times 28$).\n",
    "* **Hidden Layers:** The network contains multiple **`nn.Linear`** layers separated by **non-linear activation functions** (like `nn.ReLU`). These layers allow the network to learn complex mappings from the raw pixel values to the class probabilities.\n",
    "* **Output Layer:** The final linear layer has **10 output units**, corresponding to the 10 possible digits (0 through 9). No explicit Softmax layer is needed here because the chosen loss function handles it internally.\n",
    "\n",
    "### 3. End-to-End Training and Gradient Flow\n",
    "\n",
    "The core of the notebook implements the iterative optimization process:\n",
    "\n",
    "* **Loss Function:** The **`nn.CrossEntropyLoss`** is used. This is the standard choice for multiclass problems. Crucially, it takes the **raw model outputs (logits)** and handles both the necessary **Softmax activation** and the **Negative Log Likelihood Loss** in one step.\n",
    "* **Training Loop Mechanics:**\n",
    "    1.  **Forward Pass:** `pred = model(x_batch)`.\n",
    "    2.  **Loss Calculation:** `loss = loss_fn(pred, y_batch)`.\n",
    "    3.  **Backward Pass (Autograd):** `loss.backward()` triggers the **Autograd engine** to calculate the gradient of the loss with respect to *every* parameter in the network.\n",
    "    4.  **Parameter Update:** `optimizer.step()` adjusts all weights and biases in the direction of steepest descent.\n",
    "    5.  **Gradient Reset:** `optimizer.zero_grad()` is called at the end of the loop iteration to prevent the accumulation of gradients from previous batches.\n",
    "* **Accuracy Calculation:** Training accuracy is tracked by using `torch.argmax(pred, dim=1)` to find the predicted class and comparing it to the true label (`y_batch`).\n",
    "\n",
    "### 4. Final Evaluation and Generalization\n",
    "\n",
    "* **Test Set Evaluation:** After training is complete, the model's performance is measured on the separate **Test Set** (10,000 never-before-seen images).\n",
    "* **Result:** The final printed **Test Accuracy** provides an objective measure of the model's ability to **generalize** to new data, confirming the success of the applied PyTorch architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb8f3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754b53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5469ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= './'\n",
    "mnist_dataset_train = MNIST(root= image_path, train= True, transform= transform, download= False)\n",
    "mnist_dataset_test = MNIST(root= image_path, train= False, transform= transform, download= False)\n",
    "torch.manual_seed(1)\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(mnist_dataset_train, batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ee8c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MNIST(nn.Module):\n",
    "    def __init__(self, image_size, hidden_units= [32, 16]):\n",
    "        super().__init__()\n",
    "        input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "        layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units:\n",
    "            layers += [nn.Linear(input_size, hidden_unit), nn.ReLU()]\n",
    "            input_size = hidden_unit\n",
    "        layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "        self.module_list = nn.ModuleList(layers)\n",
    "    def forward(self, x):\n",
    "        for l in self.module_list:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "690d309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_MNIST(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = mnist_dataset_train[0][0].shape\n",
    "model = MLP_MNIST(image_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd95cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3947729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Accuracy: 0.8592\n",
      "Epoch 1, Accuracy: 0.9288\n",
      "Epoch 2, Accuracy: 0.9417\n",
      "Epoch 3, Accuracy: 0.9506\n",
      "Epoch 4, Accuracy: 0.9556\n",
      "Epoch 5, Accuracy: 0.9599\n",
      "Epoch 6, Accuracy: 0.9631\n",
      "Epoch 7, Accuracy: 0.9656\n",
      "Epoch 8, Accuracy: 0.9677\n",
      "Epoch 9, Accuracy: 0.9693\n",
      "Epoch 10, Accuracy: 0.9712\n",
      "Epoch 11, Accuracy: 0.9724\n",
      "Epoch 12, Accuracy: 0.9745\n",
      "Epoch 13, Accuracy: 0.9752\n",
      "Epoch 14, Accuracy: 0.9759\n",
      "Epoch 15, Accuracy: 0.9769\n",
      "Epoch 16, Accuracy: 0.9778\n",
      "Epoch 17, Accuracy: 0.9787\n",
      "Epoch 18, Accuracy: 0.9803\n",
      "Epoch 19, Accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    acc_hist = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        is_correct = (torch.argmax(pred, dim= 1) == y_batch).float()\n",
    "        acc_hist += is_correct.sum()\n",
    "    acc_hist /= len(train_dl.dataset)\n",
    "    print(f'Epoch {epoch}, Accuracy: {acc_hist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b14de8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "pred = model(mnist_dataset_test.data / 255.0)\n",
    "is_correct = (torch.argmax(pred, dim= 1) == mnist_dataset_test.targets).float()\n",
    "print(f'Test Accuracy: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aab34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
