{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de51ed9",
   "metadata": {},
   "source": [
    "# Transformer for Classification - DistilBERT Sentiment Analysis (Chapter 16 Application) ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the superiority and ease of use of the **Transformer Encoder** architecture for sequence classification, applying a powerful pre-trained modelâ€”**DistilBERT**â€”to the **IMDB Sentiment Analysis** task. This fully showcases the advanced, fine-tuning approach of **Chapter 16: Transformers**.\n",
    "\n",
    "### 1. The Power of Pre-trained Encoders (BERT/DistilBERT) ðŸ§ \n",
    "\n",
    "Unlike the RNN approach in Chapter 15, this method uses a vast, pre-trained model:\n",
    "\n",
    "* **DistilBERT Architecture:** This is a smaller, faster, and highly efficient version of the original **BERT (Bidirectional Encoder Representations from Transformers)** model, built entirely on the **Transformer Encoder** stack. \n",
    "* **Bidirectionality:** Unlike the GPT-2 Decoder (which is causal), BERT/DistilBERT uses **Full Bidirectional Attention**. This means the model can look at the entire input sentence simultaneously (words before and after) to build the most contextualized representation for every single word.\n",
    "* **Pre-training:** The model weights were learned on massive amounts of text data, allowing the model to already understand language, grammar, and context. The task here is merely **Fine-Tuning** those weights for the specific Sentiment Analysis task.\n",
    "\n",
    "### 2. High-Efficiency Data Pipelining with Hugging Face ðŸ“¦\n",
    "\n",
    "The notebook implements a specialized pipeline necessary for Transformer-based models:\n",
    "\n",
    "* **Specialized Tokenizer (`DistilBertTokenizerFast`):** Every Transformer model requires a tokenizer that matches its pre-training. This tokenizer performs:\n",
    "    * **Sub-word Tokenization (WordPiece):** Breaks down complex words into smaller, more common units for better vocabulary coverage.\n",
    "    * **Special Tokens:** Automatically adds the necessary tokens like `[CLS]` (Classification Token, whose final hidden state is used for the entire sequence's prediction) and `[SEP]` (Separator Token).\n",
    "* **Dataset Encoding:** The entire IMDB dataset is encoded into numerical IDs, attention masks (to ignore padding), and segment IDs.\n",
    "* **`Trainer` Utility:** The notebook employs the high-level **Hugging Face `Trainer`** and **`TrainingArguments`** classes. This utility replaces the entire manual PyTorch training loop (from Chapters 13 and 15), automatically handling:\n",
    "    * Batching and gradient accumulation.\n",
    "    * Optimization and learning rate scheduling.\n",
    "    * Evaluation and logging.\n",
    "\n",
    "### 3. Classification Head and Fine-Tuning\n",
    "\n",
    "* **Model Loading (`DistilBertForSequenceClassification`):** This loads the pre-trained DistilBERT model with an extra **Classification Head** (a linear layer) already attached to the final hidden state of the `[CLS]` token.\n",
    "* **Objective:** The training process is **Fine-Tuning**, where the vast pre-trained weights are slightly adjusted to optimize the model's performance specifically on the sentiment labels (0 or 1) of the IMDB reviews.\n",
    "* **Loss Function:** **`nn.CrossEntropyLoss`** (or its equivalent within the `transformers` library) is used for the binary classification task.\n",
    "\n",
    "This notebook demonstrates the state-of-the-art approach in NLP: leveraging large, pre-trained Transformer encoders to achieve highly accurate results in classification tasks with minimal training time compared to training an LSTM from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc16e3d-77bb-4310-a38d-56cac51901f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b6a229-d101-49ce-b618-c9a1db467631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2122a5b2750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.determinisctic = True\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8dd08e-b85a-4c89-911d-2aedb5145eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() is True else 'cpu')\n",
    "NUM_EPCCHS = 3\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940697dd-4fb4-40ff-87b6-c29a822901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz'\n",
    "filename = url.split('/')[-1]\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)\n",
    "\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open('movie_data.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddb0e7d-8580-4fa8-b4dd-d8b9dd67abc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad2c347-e5b5-44ca-a0b9-0d2122f9aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3be8b3-6da1-4a24-8cd9-df0ac3739d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation= True, padding= True)\n",
    "valid_encodngs = tokenizer(list(valid_texts), truncation= True, padding= True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation= True, padding= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df04d21-aa27-426c-ba3f-f92437a001f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx])\n",
    "                for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa60bd7c-2a4e-4aff-943a-fcda10ce2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBdataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDBdataset(valid_encodngs, valid_labels)\n",
    "test_dataset = IMDBdataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34fdc421-bdeb-446e-a0fd-d49692bd87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= 16, shuffle= True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size= 16, shuffle= False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size= 16, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c008cac7-1024-47f8-a69f-00c4d63f19fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26da823-0bb6-4962-bdab-6679e0f0b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr= 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bf1b15e-b140-4b6c-9269-41b998d42427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask= attention_mask)\n",
    "            logits = output['logits']\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (preds == labels).sum()\n",
    "\n",
    "        result = (correct_pred.float() / num_examples) * 100\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79114cb6-2afe-42eb-a533-7e194ebb8c69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/0003 | Batch 0000/2188 | Loss 0.6832\n",
      "Epoch 0001/0003 | Batch 0250/2188 | Loss 0.0818\n",
      "Epoch 0001/0003 | Batch 0500/2188 | Loss 0.1194\n",
      "Epoch 0001/0003 | Batch 0750/2188 | Loss 0.3069\n",
      "Epoch 0001/0003 | Batch 1000/2188 | Loss 0.2505\n",
      "Epoch 0001/0003 | Batch 1250/2188 | Loss 0.1905\n",
      "Epoch 0001/0003 | Batch 1500/2188 | Loss 0.1401\n",
      "Epoch 0001/0003 | Batch 1750/2188 | Loss 0.1270\n",
      "Epoch 0001/0003 | Batch 2000/2188 | Loss 0.2155\n",
      "Training Accuracy: 96.57%\n",
      "\n",
      "Valid Accuracy: 92.52%\n",
      "Time Elapsed: 81.58 min\n",
      "Epoch 0002/0003 | Batch 0000/2188 | Loss 0.0494\n",
      "Epoch 0002/0003 | Batch 0250/2188 | Loss 0.0974\n",
      "Epoch 0002/0003 | Batch 0500/2188 | Loss 0.1154\n",
      "Epoch 0002/0003 | Batch 0750/2188 | Loss 0.1304\n",
      "Epoch 0002/0003 | Batch 1000/2188 | Loss 0.0573\n",
      "Epoch 0002/0003 | Batch 1250/2188 | Loss 0.0937\n",
      "Epoch 0002/0003 | Batch 1500/2188 | Loss 0.1355\n",
      "Epoch 0002/0003 | Batch 1750/2188 | Loss 0.0119\n",
      "Epoch 0002/0003 | Batch 2000/2188 | Loss 0.4044\n",
      "Training Accuracy: 98.69%\n",
      "\n",
      "Valid Accuracy: 92.34%\n",
      "Time Elapsed: 167.24 min\n",
      "Epoch 0003/0003 | Batch 0000/2188 | Loss 0.0478\n",
      "Epoch 0003/0003 | Batch 0250/2188 | Loss 0.0037\n",
      "Epoch 0003/0003 | Batch 0500/2188 | Loss 0.0023\n",
      "Epoch 0003/0003 | Batch 0750/2188 | Loss 0.0030\n",
      "Epoch 0003/0003 | Batch 1000/2188 | Loss 0.0026\n",
      "Epoch 0003/0003 | Batch 1250/2188 | Loss 0.1328\n",
      "Epoch 0003/0003 | Batch 1500/2188 | Loss 0.0206\n",
      "Epoch 0003/0003 | Batch 1750/2188 | Loss 0.0061\n",
      "Epoch 0003/0003 | Batch 2000/2188 | Loss 0.0413\n",
      "Training Accuracy: 99.02%\n",
      "\n",
      "Valid Accuracy: 91.82%\n",
      "Time Elapsed: 249.75 min\n",
      "Total Time: 249.75 min\n",
      "Test Accuracy: 91.98%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPCCHS):\n",
    "    model.train()\n",
    "    for batch_idx , batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask= attention_mask, labels= labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if not batch_idx % 250:\n",
    "            print(f'Epoch {epoch+1:04d}/{NUM_EPOCHS:04d}'\n",
    "                  f' | Batch '\n",
    "                  f'{batch_idx:04d}/{len(train_loader)}'\n",
    "                  f' | Loss {loss:.4f}'\n",
    "                 )\n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training Accuracy: {compute_accuracy(model, train_loader, DEVICE):.2f}%')\n",
    "        print(f'\\nValid Accuracy: {compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        print(f'Time Elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Total Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test Accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82948b3-c1f4-4016-8426-77ddf70169d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased'\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ae5108-e11c-4583-bd96-a384bc04ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr= 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc84d79-6c66-4415-b872-91e417647f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir= './results',\n",
    "    num_train_epochs= 3,\n",
    "    per_device_train_batch_size= 16,\n",
    "    per_device_eval_batch_size= 16,\n",
    "    logging_dir= './logs',\n",
    "    logging_steps= 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40305da9-7edf-46df-917c-f254a6e5f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\98922\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Sep 15 01:28:52 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "956f1aad-a55c-46ff-bae1-18d3c1563ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis= -1)\n",
    "    return metric.compute(predictions= preds, references= labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a343b255-304a-4a19-ab02-fdc31be94098",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args= train_args,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset= test_dataset,\n",
    "    compute_metrics= compute_metrics,\n",
    "    optimizers= (optim, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a59825-8610-4588-ae10-15e515e6771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb3824-7a0a-41bc-874c-c18492011680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09550c-cef4-4d3e-89f3-fbac099b8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f'Test Accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b99e5-d3bb-4872-9953-7579caae23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    'test_trainer',\n",
    "    evaluation_strategy= 'epoch', ...\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
