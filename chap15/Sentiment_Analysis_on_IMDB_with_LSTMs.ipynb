{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578c476d-b42d-4940-aa0c-c22a057c53ff",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTMs and Text Preprocessing (Chapter 15 Application) üé¨\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements a complete sequence processing solution for **Sentiment Analysis** (classifying movie reviews as positive or negative) on the **IMDB Large Movie Review Dataset**. It applies the advanced recurrent architectures introduced in **Chapter 15: Processing Sequential Data with Recurrent Neural Networks (RNNs)**.\n",
    "\n",
    "### 1. Advanced Text Data Preprocessing and Pipelining üìù\n",
    "\n",
    "Handling text data requires several specialized steps:\n",
    "\n",
    "* **Dataset Loading:** Uses **`torchtext.datasets.IMDB`** for efficient loading of the large review corpus.\n",
    "* **Tokenization and Vocabulary:**\n",
    "    * **Tokenization:** Reviews are broken down into discrete units (words or sub-words).\n",
    "    * **Vocabulary:** A mapping is created from every unique word to a numerical index. This is necessary because neural networks only process numbers.\n",
    "* **Padding and Masking:** Since reviews have varying lengths, all input sequences must be standardized:\n",
    "    * **Padding:** Shorter sequences are extended with a special padding token (index 0) to match the longest sequence in the batch.\n",
    "    * **`pack_padded_sequence`:** A critical PyTorch utility is used to tell the RNN (specifically the LSTM) to **ignore the padding tokens** during computation, saving time and preventing the padding from corrupting the final hidden state.\n",
    "\n",
    "### 2. Deep Recurrent Architecture: LSTM and Bidirectionality üß†\n",
    "\n",
    "The model moves beyond the simple RNN of the first notebook to use a more powerful variant:\n",
    "\n",
    "* **Embedding Layer (`nn.Embedding`):** The first layer converts the sparse integer index of each word into a dense, low-dimensional **word vector**. This vector captures semantic relationships between words (e.g., 'good' and 'great' should have similar vectors).\n",
    "* **Long Short-Term Memory (LSTM):** The core recurrent layer. LSTMs are used instead of simple RNNs because they contain internal **gates (Input, Forget, Output)** and a **Cell State ($C_t$)** that effectively solve the **vanishing gradient problem**, allowing the network to capture dependencies that span hundreds of time steps (long-range dependencies in the review text).\n",
    "* **Bidirectional RNN (`bidirectional=True`):** The LSTM is run in both the forward and backward directions over the sequence. This allows the model to capture context from both the past and the future of a word, significantly boosting performance in text tasks.\n",
    "\n",
    "### 3. Training and Sentiment Classification\n",
    "\n",
    "* **Final Feature Combination:** The final hidden states from the forward and backward LSTMs are **concatenated** (`torch.cat`), providing a summary of the entire review's context.\n",
    "* **Classification Head:** This combined feature vector is passed through one or more fully connected layers (`nn.Linear`) to output a single prediction.\n",
    "* **Loss Function:** **`nn.BCEWithLogitsLoss`** is used, as this is a **Binary Classification** task (positive vs. negative sentiment).\n",
    "\n",
    "This notebook provides a complete and powerful blueprint for sequence modeling, demonstrating the necessary data preparation and the use of the LSTM/Bidirectional architecture for state-of-the-art text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48dee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0edebc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everthing(SEED= 1):\n",
    "    torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "549d1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everthing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b5ceff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = IMDB(split= 'train')\n",
    "test_ds = IMDB(split= 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02c3ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 create train and valid dataset\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_ds), [20000, 5000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b65609a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique tokens(words)\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall(\n",
    "    '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "    ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3689ac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size(Tokens Count): 69023\n"
     ]
    }
   ],
   "source": [
    "tokens_count = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    tokens_count.update(tokens)\n",
    "print(f'Vocab size(Tokens Count): {len(tokens_count)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9f67be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    tokens_count.items(), key= lambda x: x[1], reverse= True\n",
    ")\n",
    "\n",
    "ordered_tokens = OrderedDict(sorted_by_freq_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a2e38c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "vocab = vocab(ordered_dict= ordered_tokens)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "27583dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 457]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6cf06dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "547e4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    text_list, label_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype= torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "            text_list, batch_first= True\n",
    "    )\n",
    "        \n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "339e349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size= 4, shuffle= False, collate_fn= collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc211422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
      "             4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n",
      "           100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n",
      "             5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
      "         11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
      "          1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
      "         42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n",
      "           148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
      "          1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
      "         15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n",
      "          3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n",
      "            27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
      "           395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n",
      "          5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
      "           155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n",
      "           390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n",
      "            31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  216,   175,   724,     5,    11,    18,    10,   226,   110,    14,\n",
      "           182,    78,     8,    13,    24,   182,    78,     8,    13,   166,\n",
      "           182,    50,   150,    24,    85,     2,  4031,  5935,   107,    96,\n",
      "            28,  1867,   602,    19,    52,   162,    21,  1698,     8,     6,\n",
      "          1181,   367,     2,   351,    10,   140,   419,     4,   333,     5,\n",
      "          6022,  7136,  5055,  1209, 10892,    32,   219,     9,     2,   405,\n",
      "          1413,    13,  4031,    13,  1099,     7,    85,    19,     2,    20,\n",
      "          1018,     4,    85,   565,    34,    24,   807,    55,     5,    68,\n",
      "           658,    10,   507,     8,     4,   668,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,   121,    24,    28,    98,    74,   589,     9,   149,     2,\n",
      "          7372,  3030, 14543,  1012,   520,     2,   985,  2327,     5, 16847,\n",
      "          5479,    19,    25,    67,    76,  3478,    38,     2,  7372,     3,\n",
      "            25,    67,    76,  2951,    34,    35, 10893,   155,   449, 29495,\n",
      "         23725,    10,    67,     2,   554,    12, 14543,    67,    91,     4,\n",
      "            50,    20,    19,     8,    67,    24,  4228,     2,  2142,    37,\n",
      "            33,  3478,    87,     3,  2564,   160,   155,    11,   634,   126,\n",
      "            24,   158,    72,   286,    13,   373,     2,  4804,    19,     2,\n",
      "          7372,  6794,     6,    30,   128,    73,    48,    10,   886,     8,\n",
      "            13,    24,     4,    85,    20,    19,     8,    13,    35,   218,\n",
      "             3,   428,   710,     2,   107,   936,     7,    54,    72,   223,\n",
      "             3,    10,    96,   122,     2,   103,    54,    72,    82,     2,\n",
      "           658,   202,     2,   106,   293,   103,     7,  1193,     3,  3031,\n",
      "           708,  5760,     3,  2918,  3991,   706,  3327,   349,   148,   286,\n",
      "            13,   139,     6,     2,  1501,   750,    29,  1407,    62,    65,\n",
      "          2612,    71,    40,    14,     4,   547,     9,    62,     8,  7943,\n",
      "            71,    14,     2,  5687,     5,  4868,  3111,     6,   205,     2,\n",
      "            18,    55,  2075,     3,   403,    12,  3111,   231,    45,     5,\n",
      "           271,     3,    68,  1400,     7,  9774,   932,    10,   102,     2,\n",
      "            20,   143,    28,    76,    55,  3810,     9,  2723,     5,    12,\n",
      "            10,   379,     2,  7372,    15,     4,    50,   710,     8,    13,\n",
      "            24,   887,    32,    31,    19,     8,    13,   428],\n",
      "        [18923,     7,     4,  4753,  1669,    12,  3019,     6,     4, 13906,\n",
      "           502,    40,    25,    77,  1588,     9,   115,     6, 21713,     2,\n",
      "            90,   305,   237,     9,   502,    33,    77,   376,     4, 16848,\n",
      "           847,    62,    77,   131,     9,     2,  1580,   338,     5, 18923,\n",
      "            32,     2,  1980,    49,   157,   306, 21713,    46,   981,     6,\n",
      "         10298,     2, 18924,   125,     9,   502,     3,   453,     4,  1852,\n",
      "           630,   407,  3407,    34,   277,    29,   242,     2, 20200,     5,\n",
      "         18923,    77,    95,    41,  1833,     6,  2105,    56,     3,   495,\n",
      "           214,   528,     2,  3479,     2,   112,     7,   181,  1813,     3,\n",
      "           597,     5,     2,   156,   294,     4,   543,   173,     9,  1562,\n",
      "           289, 10038,     5,     2,    20,    26,   841,  1392,    62,   130,\n",
      "           111,    72,   832,    26,   181, 12402,    15,    69,   183,     6,\n",
      "            66,    55,   936,     5,     2,    63,     8,     7,    43,     4,\n",
      "            78, 23726, 15995,    13,    20,    17,   800,     5,   392,    59,\n",
      "          3992,     3,   371,   103,  2596,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([165,  86, 218, 145])\n",
      "torch.Size([4, 218])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, lengths_batch = next(iter(dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(lengths_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b027b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle= True, collate_fn= collate_batch)\n",
    "val_dl = DataLoader(valid_dataset, batch_size, shuffle= False, collate_fn= collate_batch)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle= False, collate_fn= collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "61fbf402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7039, -0.8321, -0.4651],\n",
      "         [-0.3203,  2.2408,  0.5566],\n",
      "         [-0.7106, -0.2959,  0.8356],\n",
      "         [-0.3203,  2.2408,  0.5566]],\n",
      "\n",
      "        [[-0.3203,  2.2408,  0.5566],\n",
      "         [ 0.0946, -0.3531,  0.9124],\n",
      "         [-0.4643,  0.3046,  0.7046],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings= 10,\n",
    "    embedding_dim= 3,\n",
    "    padding_idx= 0\n",
    ")\n",
    "print(embedding(torch.LongTensor([[1, 2, 5, 2], [2, 3, 4, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "56dcc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers= 2, batch_first= True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "46db2239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN(64, 32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a2616873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3183],\n",
      "        [ 0.1230],\n",
      "        [ 0.1772],\n",
      "        [-0.1052],\n",
      "        [-0.1259]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.randn(5, 3, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "794c0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings= vocab_size, embedding_dim= emb_dim, padding_idx= 0)\n",
    "        self.rnn = nn.LSTM(emb_dim, rnn_hidden_size, num_layers= 2, batch_first= True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu(), enforce_sorted= False, batch_first= True)\n",
    "        out , (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2eb6491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
      "  (rnn): LSTM(20, 64, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "model = RNNModel(vocab_size, emb_dim, rnn_hidden_size, fc_hidden_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f9a7d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2d141943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    train_acc, train_loss = 0., 0.\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * label_batch.size(0)\n",
    "        is_correct = ((pred >= 0.5).float() == label_batch).float()\n",
    "        train_acc += is_correct.sum().item()\n",
    "    train_loss /= len(dataloader.dataset)\n",
    "    train_acc /= len(dataloader.dataset)\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "380b5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            val_loss += loss.item() * label_batch.size(0)\n",
    "            is_correct = ((pred >= 0.5).float() == label_batch).float()\n",
    "            val_acc += is_correct.sum().item()\n",
    "        val_loss /= len(dataloader.dataset)\n",
    "        val_acc /= len(dataloader.dataset)\n",
    "        \n",
    "        return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3450bee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1\tTrain:\tAccuracy: 1.0 Loss: 0.018987190446181922\n",
      "                  \tValid:\tAccuracy: 1.0   Loss: 2.0846029452513904e-05\n",
      "Epochs 2\tTrain:\tAccuracy: 1.0 Loss: 1.0463686278671957e-05\n",
      "                  \tValid:\tAccuracy: 1.0   Loss: 5.122671836579684e-06\n",
      "Epochs 3\tTrain:\tAccuracy: 1.0 Loss: 3.283095588631113e-06\n",
      "                  \tValid:\tAccuracy: 1.0   Loss: 2.0333269636466865e-06\n",
      "Epochs 4\tTrain:\tAccuracy: 1.0 Loss: 1.4813292138569522e-06\n",
      "                  \tValid:\tAccuracy: 1.0   Loss: 1.0728830375228426e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 3\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(val_dl)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTrain:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[140], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(text_batch, lengths)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, label_batch)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m label_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(train_dl)\n",
    "    val_loss, val_acc = evaluate(val_dl)\n",
    "    \n",
    "    print(f'Epochs {epoch + 1}\\tTrain:\\tAccuracy: {train_acc} Loss: {train_loss}')\n",
    "    print(f'                  \\tValid:\\tAccuracy: {val_acc}   Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelBidirectional(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_index= 0)\n",
    "        self.rnn = nn.LSTM(emb_size, rnn_hidden_size, batch_first= True, bidirectional= True)\n",
    "        self.fc1 = nn.Linear(2* rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        out = nn.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu(), enforce_sorted= False, batch_first= True\n",
    "        )\n",
    "        out = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :],\n",
    "                         hidden[-1, :, :]), dim= 1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
